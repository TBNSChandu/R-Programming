library(caret)
data1 <- read.csv("JDT.csv")
data1$defect <- as.factor(data1$defect)
M1_data1_model <- train(defect ~ loc + bf, data = data1, method = "glm", family = "binomial")
data1_probability <- predict(M1_data1_model, data1, type = "prob")[, 2]
cutoff_sequence <- seq(0.1, 0.9, by = 0.1)
data1_metrics <- data.frame(Cutoff = numeric(), Recall = numeric(), Precision = numeric(), Accuracy = numeric(), F_Score = numeric())
for (cutoff in cutoff_sequence) {
data1$Predicted_Defect <- ifelse(data1_probability > cutoff, 1, 0)
data1$Predicted_Defect <- factor(data1$Predicted_Defect, levels = levels(data1$defect))
resultant_matrix_data1 <- confusionMatrix(data = data1$Predicted_Defect, reference = data1$defect)
data1_recall <- resultant_matrix_data1$byClass["Recall"]
data1_precision <- resultant_matrix_data1$byClass["Precision"]
data1_accuracy <- sum(data1$Predicted_Defect == data1$defect) / nrow(data1)
data1_f_score <- 2 * (data1_precision * data1_recall) / (data1_precision + data1_recall)
data1_metrics <- rbind(data1_metrics, data.frame(Cutoff = cutoff, Recall = data1_recall, Precision = data1_precision, Accuracy = data1_accuracy, F_Score = data1_f_score))
}
highest_threshold_data1 <- data1_metrics[which.max(data1_metrics$F_Score),]
print(highest_threshold_data1)
data2 <- read.csv("PDE.csv")
data2$defect <- as.factor(data2$defect)
M1_data2_model <- train(defect ~ loc + bf, data = data2, method = "glm", family = "binomial")
data2_probability <- predict(M1_data2_model, data2, type = "prob")[, 2]
cutoff_sequence <- seq(0.1, 0.9, by = 0.1)
data2_metrics <- data.frame(Cutoff = numeric(), Recall = numeric(), Precision = numeric(), Accuracy = numeric(), F_Score = numeric())
for (cutoff in cutoff_sequence) {
data2$Predicted_Defect <- ifelse(data2_probability > cutoff, 1, 0)
data2$Predicted_Defect <- factor(data2$Predicted_Defect, levels = levels(data2$defect))
resultant_matrix_data2 <- confusionMatrix(data = data2$Predicted_Defect, reference = data2$defect)
data2_recall <- resultant_matrix_data2$byClass["Recall"]
data2_precision <- resultant_matrix_data2$byClass["Precision"]
data2_accuracy <- sum(data2$Predicted_Defect == data2$defect) / nrow(data2)
data2_f_score <- 2 * (data2_precision * data2_recall) / (data2_precision + data2_recall)
data2_metrics <- rbind(data2_metrics, data.frame(Cutoff = cutoff, Recall = data2_recall, Precision = data2_precision, Accuracy = data2_accuracy, F_Score = data2_f_score))
}
highest_threshold_data2 <- data2_metrics[which.max(data2_metrics$F_Score),]
print(highest_threshold_data2)

library(caret)
data1 <- read.csv("JDT.csv")
data2 <- read.csv("PDE.csv")
data1$defect <- as.factor(data1$defect)
M1_model_train_data1 <- train(defect ~ loc + bf, data = data1, method = "glm", family = "binomial")
data2$defect <- as.factor(data2$defect)
data2_prob <- predict(M1_model_train_data1, data2, type = "prob")[, 2]
cutoff_sequence <- seq(0.1, 0.9, by = 0.1)
output_metrics <- data.frame(Cutoff = numeric(), Recall = numeric(), Precision = numeric(), Accuracy = numeric(), F_Score = numeric())
for (cutoff in cutoff_sequence) {
data2$Predicted_Defect <- ifelse(data2_prob > cutoff, 1, 0)
data2$Predicted_Defect <- factor(data2$Predicted_Defect, levels = levels(data2$defect))
resultant_matrix <- confusionMatrix(data = data2$Predicted_Defect, reference = data2$defect)
recall <- resultant_matrix$byClass["Recall"]
precision <- resultant_matrix$byClass["Precision"]
accuracy <- sum(data2$Predicted_Defect == data2$defect) / nrow(data2)
f_score <- 2 * (precision * recall) / (precision + recall)
output_metrics <- rbind(output_metrics, data.frame(Cutoff = cutoff, Recall = recall, Precision = precision, Accuracy = accuracy, F_Score = f_score))
}
highest_threshold <- output_metrics[which.max(output_metrics$F_Score),]
print(highest_threshold)
library(caret)
data1 <- read.csv("JDT.csv")
data2 <- read.csv("PDE.csv")
data2$defect <- as.factor(data2$defect)
M1_model_train_data2 <- train(defect ~ loc + bf, data = data2, method = "glm", family = "binomial")
data1$defect <- as.factor(data1$defect)
data1_prob <- predict(M1_model_train_data2, data1, type = "prob")[, 2]
cutoff_sequence <- seq(0.1, 0.9, by = 0.1)
output_metrics <- data.frame(Cutoff = numeric(), Recall = numeric(), Precision = numeric(), Accuracy = numeric(), F_Score = numeric())
for (cutoff in cutoff_sequence) {
data1$Predicted_Defect <- ifelse(data1_prob > cutoff, 1, 0)
data1$Predicted_Defect <- factor(data1$Predicted_Defect, levels = levels(data1$defect))
resultant_matrix <- confusionMatrix(data = data1$Predicted_Defect, reference = data1$defect)
recall <- resultant_matrix$byClass["Recall"]
precision <- resultant_matrix$byClass["Precision"]
accuracy <- sum(data1$Predicted_Defect == data1$defect) / nrow(data1)
f_score <- 2 * (precision * recall) / (precision + recall)
output_metrics <- rbind(output_metrics, data.frame(Cutoff = cutoff, Recall = recall, Precision = precision, Accuracy = accuracy, F_Score = f_score))
}
highest_threshold <- output_metrics[which.max(output_metrics$F_Score),]
print(highest_threshold)

library(caret)
data1 <- read.csv("JDT.csv")
data2 <- read.csv("PDE.csv")
data1$defect <- as.factor(data1$defect)
data2$defect <- as.factor(data2$defect)
T1_model_train_data1 <- train(defect ~ loc + bf, data = data1, method = "rpart")
cutoff_sequence <- seq(0.1, 0.9, by = 0.1)
output_metrics <- data.frame(Cutoff = numeric(), Recall = numeric(), Precision = numeric(), Accuracy = numeric(), F_Score = numeric())
for (cutoff in cutoff_sequence) {
data2_prob <- predict(T1_model_train_data1, data2, type = "prob")[, 2]
data2$Predicted_Defect <- ifelse(data2_prob > cutoff, 1, 0)
data2$Predicted_Defect <- factor(data2$Predicted_Defect, levels = levels(data2$defect))
resultant_matrix <- confusionMatrix(data = data2$Predicted_Defect, reference = data2$defect)
recall <- resultant_matrix$byClass["Recall"]
precision <- resultant_matrix$byClass["Precision"]
accuracy <- sum(data2$Predicted_Defect == data2$defect) / nrow(data2)
f_score <- 2 * (precision * recall) / (precision + recall)
output_metrics <- rbind(output_metrics, data.frame(Cutoff = cutoff, Recall = recall, Precision = precision, Accuracy = accuracy, F_Score = f_score))
}
highest_threshold <- output_metrics[which.max(output_metrics$F_Score),]
print(highest_threshold)
library(caret)
data1 <- read.csv("JDT.csv")
data2 <- read.csv("PDE.csv")
data1$defect <- as.factor(data1$defect)
data2$defect <- as.factor(data2$defect)
T1_model_train_data2 <- train(defect ~ loc + bf, data = data2, method = "rpart")
cutoff_sequence <- seq(0.1, 0.9, by = 0.1)
output_metrics <- data.frame(Cutoff = numeric(), Recall = numeric(), Precision = numeric(), Accuracy = numeric(), F_Score = numeric())
for (cutoff in cutoff_sequence) {
data1_prob <- predict(T1_model_train_data2, data1, type = "prob")[, 2]
data1$Predicted_Defect <- ifelse(data1_prob > cutoff, 1, 0)
data1$Predicted_Defect <- factor(data1$Predicted_Defect, levels = levels(data1$defect))
resultant_matrix <- confusionMatrix(data = data1$Predicted_Defect, reference = data1$defect)
recall <- resultant_matrix$byClass["Recall"]
precision <- resultant_matrix$byClass["Precision"]
accuracy <- sum(data1$Predicted_Defect == data1$defect) / nrow(data1)
f_score <- 2 * (precision * recall) / (precision + recall)
output_metrics <- rbind(output_metrics, data.frame(Cutoff = cutoff, Recall = recall, Precision = precision, Accuracy = accuracy, F_Score = f_score))
}
highest_threshold <- output_metrics[which.max(output_metrics$F_Score),]
print(highest_threshold)
 
library(caret)
data1 <- read.csv("JDT.csv")
data1$defect <- as.factor(data1$defect)
T1_model_data1 <- train(defect ~ loc + bf, data = data1, method = "rpart")
cutoff_sequence <- seq(0.1, 0.9, by = 0.1)
output_metrics <- data.frame(Cutoff = numeric(), Recall = numeric(), Precision = numeric(), Accuracy = numeric(), F_Score = numeric())
for (cutoff in cutoff_sequence) {
data1_prob <- predict(T1_model_data1, data1, type = "prob")[, 2]
data1$Predicted_Defect <- ifelse(data1_prob > cutoff, 1, 0)
data1$Predicted_Defect <- factor(data1$Predicted_Defect, levels = levels(data1$defect))
resultant_matrix <- confusionMatrix(data = data1$Predicted_Defect, reference = data1$defect)
recall <- resultant_matrix$byClass["Recall"]
precision <- resultant_matrix$byClass["Precision"]
accuracy <- sum(data1$Predicted_Defect == data1$defect) / nrow(data1)
f_score <- 2 * (precision * recall) / (precision + recall)
output_metrics <- rbind(output_metrics, data.frame(Cutoff = cutoff, Recall = recall, Precision = precision, Accuracy = accuracy, F_Score = f_score))
}
highest_threshold <- output_metrics[which.max(output_metrics$F_Score),]
print(highest_threshold)
library(caret)
data2 <- read.csv("PDE.csv")
data2$defect <- as.factor(data2$defect)
T1_model_train_data2 <- train(defect ~ loc + bf, data = data2, method = "rpart")
cutoff_sequence <- seq(0.1, 0.9, by = 0.1)
output_metrics <- data.frame(Cutoff = numeric(), Recall = numeric(), Precision = numeric(), Accuracy = numeric(), F_Score = numeric())
for (cutoff in cutoff_sequence) {
data2_prob <- predict(T1_model_train_data2, data2, type = "prob")[, 2]
data2$Predicted_Defect <- ifelse(data2_prob > cutoff, 1, 0)
data2$Predicted_Defect <- factor(data2$Predicted_Defect, levels = levels(data2$defect))
resultant_matrix <- confusionMatrix(data = data2$Predicted_Defect, reference = data2$defect)
recall <- resultant_matrix$byClass["Recall"]
precision <- resultant_matrix$byClass["Precision"]
accuracy <- sum(data2$Predicted_Defect == data2$defect) / nrow(data2)
f_score <- 2 * (precision * recall) / (precision + recall)
output_metrics <- rbind(output_metrics, data.frame(Cutoff = cutoff, Recall = recall, Precision = precision, Accuracy = accuracy, F_Score = f_score))
}
highest_threshold <- output_metrics[which.max(output_metrics$F_Score),]
print(highest_threshold)

library(caret)
data1 <- read.csv("JDT.csv")
data1$defect <- as.factor(data1$defect)
repetetion_count <- 30
recall_vector <- numeric()
precision_vector <- numeric()
accuracy_vector <- numeric()
f_score_vector <- numeric()
M1_data1_model <- train(defect ~ loc + bf, data = data1, method = "glm", family = "binomial")
optimal_threshold <- 0.5
for (i in 1:repetetion_count) {
set.seed(i)
indices <- sample(1:nrow(data1), nrow(data1) / 2)
data1_train <- data1[indices, ]
data1_test <- data1[-indices, ]
data1_test_prob <- predict(M1_data1_model, data1_test, type = "prob")[, 2]
data1_test$Predicted_Defect <- ifelse(data1_test_prob > optimal_threshold, 1, 0)
data1_test$Predicted_Defect <- factor(data1_test$Predicted_Defect, levels = levels(data1_test$defect))
resultant_matrix <- confusionMatrix(data = data1_test$Predicted_Defect, reference = data1_test$defect)
recall <- resultant_matrix$byClass["Recall"]
precision <- resultant_matrix$byClass["Precision"]
accuracy <- sum(data1_test$Predicted_Defect == data1_test$defect) / nrow(data1_test)
f_score <- 2 * (precision * recall) / (precision + recall)
recall_vector <- c(recall_vector, recall)
precision_vector <- c(precision_vector, precision)
accuracy_vector <- c(accuracy_vector, accuracy)
f_score_vector <- c(f_score_vector, f_score)
}
avg_recall <- mean(recall_vector)
avg_precision <- mean(precision_vector)
avg_accuracy <- mean(accuracy_vector)
avg_f_score <- mean(f_score_vector)
stddev_recall <- sd(recall_vector)
stddev_precision <- sd(precision_vector)
stddev_accuracy <- sd(accuracy_vector)
stddev_f_score <- sd(f_score_vector)
required_output <- data.frame(
Metric = c("Recall", "Precision", "Accuracy", "F-Score"),
Average = c(avg_recall, avg_precision, avg_accuracy, avg_f_score),
StandardDeviation = c(stddev_recall, stddev_precision, stddev_accuracy, stddev_f_score)
)
print(required_output)
library(caret)
data2 <- read.csv("PDE.csv")
data2$defect <- as.factor(data2$defect)
repetition_count <- 30
recall_vector <- numeric()
precision_vector <- numeric()
accuracy_vector <- numeric()
f_score_vector <- numeric()
M1_data2_model <- train(defect ~ loc + bf, data = data2, method = "glm", family = "binomial")
optimal_threshold <- 0.7
for (i in 1:repetition_count) {
set.seed(i)
indices <- sample(1:nrow(data2), nrow(data2) / 2)
data2_train <- data2[indices, ]
data2_test <- data2[-indices, ]
data2_test_prob <- predict(M1_data2_model, data2_test, type = "prob")[, 2]
data2_test$Predicted_Defect <- ifelse(data2_test_prob > optimal_threshold, 1, 0)
data2_test$Predicted_Defect <- factor(data2_test$Predicted_Defect, levels = levels(data2_test$defect))
resultant_matrix <- confusionMatrix(data = data2_test$Predicted_Defect, reference = data2_test$defect)
recall <- resultant_matrix$byClass["Recall"]
precision <- resultant_matrix$byClass["Precision"]
accuracy <- sum(data2_test$Predicted_Defect == data2_test$defect) / nrow(data2_test)
f_score <- 2 * (precision * recall) / (precision + recall)
recall_vector <- c(recall_vector, recall)
precision_vector <- c(precision_vector, precision)
accuracy_vector <- c(accuracy_vector, accuracy)
f_score_vector <- c(f_score_vector, f_score)
}
avg_recall <- mean(recall_vector)
avg_precision <- mean(precision_vector)
avg_accuracy <- mean(accuracy_vector)
avg_f_score <- mean(f_score_vector)
stddev_recall <- sd(recall_vector)
stddev_precision <- sd(precision_vector)
stddev_accuracy <- sd(accuracy_vector)
stddev_f_score <- sd(f_score_vector)
required_output <- data.frame(
Metric = c("Recall", "Precision", "Accuracy", "F-Score"),
Average = c(avg_recall, avg_precision, avg_accuracy, avg_f_score),
StandardDeviation = c(stddev_recall, stddev_precision, stddev_accuracy, stddev_f_score)
)
print(required_output)

library(caret)
data1 <- read.csv("JDT.csv")
data1$defect <- as.factor(data1$defect)
repetition_count <- 30
recall_vector <- numeric()
precision_vector <- numeric()
accuracy_vector <- numeric()
f_score_vector <- numeric()
T1_data1_model <- train(defect ~ loc + bf, data = data1, method = "rpart")
optimal_threshold <- 0.2
for (i in 1:repetition_count) {
set.seed(i)
indices <- sample(1:nrow(data1), nrow(data1) / 2)
data1_train <- data1[indices, ]
data1_test <- data1[-indices, ]
data1_test_prob <- predict(T1_data1_model, data1_test, type = "prob")[, 2]
data1_test$Predicted_Defect <- ifelse(data1_test_prob > optimal_threshold, 1, 0)
data1_test$Predicted_Defect <- factor(data1_test$Predicted_Defect, levels = levels(data1_test$defect))
resultant_matrix <- confusionMatrix(data = data1_test$Predicted_Defect, reference = data1_test$defect)
recall <- resultant_matrix$byClass["Recall"]
precision <- resultant_matrix$byClass["Precision"]
accuracy <- sum(data1_test$Predicted_Defect == data1_test$defect) / nrow(data1_test)
f_score <- 2 * (precision * recall) / (precision + recall)
recall_vector <- c(recall_vector, recall)
precision_vector <- c(precision_vector, precision)
accuracy_vector <- c(accuracy_vector, accuracy)
f_score_vector <- c(f_score_vector, f_score)
}
avg_recall <- mean(recall_vector)
avg_precision <- mean(precision_vector)
avg_accuracy <- mean(accuracy_vector)
avg_f_score <- mean(f_score_vector)
stddev_recall <- sd(recall_vector)
stddev_precision <- sd(precision_vector)
stddev_accuracy <- sd(accuracy_vector)
stddev_f_score <- sd(f_score_vector)
required_output <- data.frame(
Metric = c("Recall", "Precision", "Accuracy", "F-Score"),
Average = c(avg_recall, avg_precision, avg_accuracy, avg_f_score),
StandardDeviation = c(stddev_recall, stddev_precision, stddev_accuracy, stddev_f_score)
)
print(required_output)
library(caret)
data2 <- read.csv("PDE.csv")
data2$defect <- as.factor(data2$defect)
repetition_count <- 30
recall_vector <- numeric()
precision_vector <- numeric()
accuracy_vector <- numeric()
f_score_vector <- numeric()
T1_data2_model <- train(defect ~ loc + bf, data = data2, method = "rpart")
optimal_threshold <- 0.2
for (i in 1:repetition_count) {
set.seed(i)
indices <- sample(1:nrow(data2), nrow(data2) / 2)
data2_train <- data2[indices, ]
data2_test <- data2[-indices, ]
data2_test_prob <- predict(T1_data2_model, data2_test, type = "prob")[, 2]
data2_test$Predicted_Defect <- ifelse(data2_test_prob > optimal_threshold, 1, 0)
data2_test$Predicted_Defect <- factor(data2_test$Predicted_Defect, levels = levels(data2_test$defect))
resultant_matrix <- confusionMatrix(data = data2_test$Predicted_Defect, reference = data2_test$defect)
recall <- resultant_matrix$byClass["Recall"]
precision <- resultant_matrix$byClass["Precision"]
accuracy <- sum(data2_test$Predicted_Defect == data2_test$defect) / nrow(data2_test)
f_score <- 2 * (precision * recall) / (precision + recall)
recall_vector <- c(recall_vector, recall)
precision_vector <- c(precision_vector, precision)
accuracy_vector <- c(accuracy_vector, accuracy)
f_score_vector <- c(f_score_vector, f_score)
}
avg_recall <- mean(recall_vector)
avg_precision <- mean(precision_vector)
avg_accuracy <- mean(accuracy_vector)
avg_f_score <- mean(f_score_vector)
stddev_recall <- sd(recall_vector)
stddev_precision <- sd(precision_vector)
stddev_accuracy <- sd(accuracy_vector)
stddev_f_score <- sd(f_score_vector)
required_output <- data.frame(
Metric = c("Recall", "Precision", "Accuracy", "F-Score"),
Average = c(avg_recall, avg_precision, avg_accuracy, avg_f_score),
StandardDeviation = c(stddev_recall, stddev_precision, stddev_accuracy, stddev_f_score)
)
print(required_output)

library(caret)
required_actual_total_bugs <- function(predicted_data, actual_values) {
n <- length(predicted_data)
cutoff_top_20 <- round(0.2 * n)
sorted_indices <- order(predicted_data, decreasing = TRUE)
indices_top_20 <- sorted_indices[1:cutoff_top_20]
actual_total_bugs_in_top_20 <- sum(actual_values[indices_top_20])
return(actual_total_bugs_in_top_20)
}
top_20_required_model <- function(predicted_data, actual_values) {
actual_bugs <- sum(actual_values)
actual_total_bugs_in_top_20_percent <- required_actual_total_bugs(predicted_data, actual_values)
required_effectiveness <- actual_total_bugs_in_top_20_percent / actual_bugs
return(required_effectiveness)
}
required_M2_cross_validation <- function(data) {
set.seed(450)
output_data <- numeric(30)
for (i in 1:30) {
train_indices <- createDataPartition(data$bug, p = 0.5, list = FALSE)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
M2_model <- lm(bug ~ loc + bf, data = train_data)
predicted_data <- predict(M2_model, newdata = test_data)
required_effectiveness <- top_20_required_model(predicted_data, test_data$bug)
output_data[i] <- required_effectiveness
}
return(output_data)
}
data1 <- read.csv("JDT.csv")
output1 <- required_M2_cross_validation(data1)
print(output1)
average_effectiveness <- mean(output1)
cat("Average Effectiveness:", average_effectiveness, "\n")
data2 <- read.csv("PDE.csv")
output2 <- required_M2_cross_validation(data2)
print(output2)
average_effectiveness <- mean(output2)
cat("Average Effectiveness:", average_effectiveness, "\n")

library(caret)
required_actual_total_bugs <- function(predicted_data, actual_values) {
n <- length(predicted_data)
cutoff_top_20 <- round(0.2 * n)
sorted_indices <- order(predicted_data, decreasing = TRUE)
indices_top_20 <- sorted_indices[1:cutoff_top_20]
actual_total_bugs_in_top_20 <- sum(actual_values[indices_top_20])
return(actual_total_bugs_in_top_20)
}
top_20_required_model <- function(predicted_data, actual_values) {
actual_bugs <- sum(actual_values)
actual_total_bugs_in_top_20_percent <- required_actual_total_bugs(predicted_data, actual_values)
required_effectiveness <- actual_total_bugs_in_top_20_percent / actual_bugs
return(required_effectiveness)
}
required_T2_cross_validation <- function(data) {
set.seed(450)
output_data <- numeric(30)
for (i in 1:30) {
train_indices <- createDataPartition(data$bug, p = 0.5, list = FALSE)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
T2_model <- train(bug ~ loc + bf, data = train_data, method = "rpart")
predicted_data <- predict(T2_model, newdata = test_data)
required_effectiveness <- top_20_required_model(predicted_data, test_data$bug)
output_data[i] <- required_effectiveness
}
return(output_data)
}
data1 <- read.csv("JDT.csv")
output1 <- required_T2_cross_validation(data1)
print(output1)
average_effectiveness <- mean(output1)
cat("Average Effectiveness on JDT:", average_effectiveness, "\n")
data2 <- read.csv("PDE.csv")
output2 <-  required_T2_cross_validation(data2)
print(output2)
average_effectiveness <- mean(output2)
cat("Average Effectiveness on PDE:", average_effectiveness, "\n")

savehistory("C:/Users/cscha/OneDrive/Desktop/Software analytics/Assignment8/Assignment8.Rhistory")