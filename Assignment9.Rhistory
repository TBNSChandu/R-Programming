training_data <- c(
"open seek read read write close",
"open read write read write read write close",
"open write write write close",
"open read close open write close",
"seek read write",
"open eof read close",
"open seek read seek read close",
"eof read",
"open seek write write write close",
"open seek read write seek read write close"
)
training_data <- paste("ST", training_data, "ED", sep = " ")
tokens <- unlist(strsplit(training_data, split = " "))
token_counts <- table(tokens)
unigram_probs <- token_counts / sum(token_counts)
print(unigram_probs)

deriveBigramProbabilities <- function(training_data) {
training_data <- paste("ST", training_data, "ED", sep = " ")
tokens <- unlist(strsplit(training_data, split = " "))
count_Bigram <- matrix(0, nrow = length(unique(tokens)), ncol = length(unique(tokens)),
dimnames = list(unique(tokens), unique(tokens)))
for (i in 1:(length(tokens) - 1)) {
count_Bigram[tokens[i], tokens[i + 1]] <- count_Bigram[tokens[i], tokens[i + 1]] + 1
}
bigramProbabilities <- count_Bigram / rowSums(count_Bigram)
return(bigramProbabilities)
}
training_data <- c(
"open seek read read write close",
"open read write read write read write close",
"open write write write close",
"open read close open write close",
"seek read write",
"open eof read close",
"open seek read seek read close",
"eof read",
"open seek write write write close",
"open seek read write seek read write close"
)
bigramProbabilities <- deriveBigramProbabilities(training_data)
print(bigramProbabilities, digits = 4)

bigram_M2_model <- matrix(
c(
0, 0.8000, 0.10000, 0.00000, 0.0000, 0.0000, 0.00000, 0.1000,
0, 0.0000, 0.44444, 0.22222, 0.2222, 0.0000, 0.00000, 0.1111,
0, 0.0000, 0.00000, 0.85714, 0.1429, 0.0000, 0.00000, 0.0000,
0, 0.0000, 0.07692, 0.07692, 0.5385, 0.2308, 0.07692, 0.0000,
0, 0.0000, 0.07143, 0.14286, 0.2857, 0.4286, 0.07143, 0.0000,
0, 0.1111, 0.00000, 0.00000, 0.0000, 0.0000, 0.88889, 0.0000,
1, 0.0000, 0.00000, 0.00000, 0.0000, 0.0000, 0.00000, 0.0000,
0, 0.0000, 0.00000, 1.00000, 0.0000, 0.0000, 0.00000, 0.0000
),
nrow = 8, byrow = TRUE,
dimnames = list(
c("ST", "open", "seek", "read", "write", "close", "ED", "eof"),
c("ST", "open", "seek", "read", "write", "close", "ED", "eof")
)
)
present_tokens <- c("open", "write")
probabilities <- bigram_M2_model[, present_tokens]
row_index <- match(present_tokens[1], colnames(bigram_M2_model))
probabilities <- bigram_M2_model[row_index, ]
output_most_likely_tokens <- names(sort(probabilities, decreasing = TRUE)[1:2])
cat("Two most likely tokens following 'open write':", output_most_likely_tokens[1], "and", output_most_likely_tokens[2], "\n")

data8 <- read.csv("question8.csv")
lm_model_M8<- lm(y ~ x1 + x2, data = data8)
summary(lm_model_M8)
predict(lm_model_M8, data.frame(x1=0.1, x2=0.7))

install.packages("nnet")
library(nnet)
data9 <-  read.csv("question9.csv")
y1_model <- multinom(y1 ~ x1+x2+x3+x4, data= data9)
summary(y1_model)
y2_model <- multinom(y2 ~ x1 + x2 + x3 + x4,  data = data9)
summary(y2_model)
present_data <- data.frame(x1 = 0.1, x2 = 0.5, x3 = 0.5, x4 = 0.7)
prob_y1 <- predict(y1_model, newdata=present_data, type="probs")
prob_y2 <- predict(y2_model, newdata=present_data, type="probs")
print(prob_y1)
print(prob_y2)
y1_most_likely <- names(prob_y1)[which.max(prob_y1)]
y2_most_likely <- names(prob_y2)[which.max(prob_y2)]
print(y1_most_likely)
print(y2_most_likely)

install.packages("neuralnetwork")
library(neuralnetwork)
data10 <- read.csv("question10.csv")
net1 <- neuralnet(y ~ ., data = data10, hidden = 1)
plot(net1)
net2 <- neuralnet(y ~ ., data = data10, hidden = 2)
plot(net2)
net3 <- neuralnet(y ~ ., data = data10, hidden = 3)
plot(net3)
compute(net3, data.frame(x1=0.1,x2=0.5,x3=0.5,x4=0.7))

library(neuralnetwork)
data11 <- read.csv("question11.csv")
net1 = neuralnet(y~., data=data11, hidden=1)
plot(net1)
net2 = neuralnet(y~., data=data11, hidden=2)
plot(net2)
net3 = neuralnet(y~., data=data11, hidden=3)
plot(net3)
compute(net3,data.frame(x1=0.1,x2=0.5,x3=0.9,x4=0.5))

savehistory("C:/Users/cscha/OneDrive/Desktop/Software analytics/Assignment9/Assignment9.Rhistory")
